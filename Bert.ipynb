{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, PackedSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.eps = 1e-12\n",
    "        \n",
    "        # Defining the maximum number of the token, embeddings and positions\n",
    "        self.vocab_size = 30522\n",
    "        self.sent_size = 2\n",
    "        self.pos_size = 512\n",
    "        \n",
    "        # Defining the embedding dimensions of the tokens, sentences and positions\n",
    "        self.emb_dim = 768\n",
    "        \n",
    "        # Embedding layer dropout rate\n",
    "        self.emb_drop_rate = 0.5\n",
    "        \n",
    "        # Attention Layer\n",
    "        self.num_heads = 12\n",
    "        self.attn_drop_rate = 0.1\n",
    "        \n",
    "        # FeedForward Lyaer\n",
    "        self.hidden_dim = 3072\n",
    "        self.fc_drop_rate = 0.1\n",
    "        \n",
    "        # Encoder\n",
    "        self.num_encoders = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        \n",
    "        self.token_embeddings = nn.Embedding(num_embeddings=config.vocab_size, embedding_dim=config.emb_dim)\n",
    "        self.sentence_embeddings = nn.Embedding(num_embeddings=config.sent_size, embedding_dim=config.emb_dim)\n",
    "        self.positional_embeddings = nn.Embedding(num_embeddings=config.pos_size, embedding_dim=config.emb_dim)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=config.emb_dim, eps=config.eps)\n",
    "        self.emb_dropout = nn.Dropout(p=config.emb_drop_rate)\n",
    "        \n",
    "    def forward(self, token_ip, sent_ip, pos_ip):\n",
    "        \n",
    "        token_emb = self.token_embeddings(token_ip)\n",
    "        sent_emb = self.sentence_embeddings(sent_ip)\n",
    "        pos_emb = self.positional_embeddings(pos_ip)\n",
    "        \n",
    "        embeddings = token_emb + sent_emb + pos_emb\n",
    "        \n",
    "        normalized_emb = self.layer_norm(embeddings)\n",
    "        output = self.emb_dropout(normalized_emb)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super(MultiHeadSelfAttentionLayer, self).__init__()\n",
    "        \n",
    "        #config.num_heads should divide config.emb_dim\n",
    "        self.each_head_dim = config.emb_dim/config.num_heads\n",
    "        \n",
    "        self.queries = nn.Linear(in_features=config.emb_dim, out_features=config.emb_dim)\n",
    "        self.keys = nn.Linear(in_features=config.emb_dim, out_features=config.emb_dim)\n",
    "        self.values = nn.Linear(in_features=config.emb_dim, out_features=config.emb_dim)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(p=config.attn_drop_rate)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=config.emb_dim, out_features=config.emb_dim)\n",
    "        \n",
    "    \n",
    "    def forwards(self, X):\n",
    "        \n",
    "        Q = self.queries(X)\n",
    "        K = self.keys(X)\n",
    "        V = self.values(X)\n",
    "        \n",
    "        old_shape = list(Q.shape())\n",
    "        new_shape = old_shape[:-1] + [config.num_heads, self.each_head_dim]\n",
    "        \n",
    "        Q = torch.transpose(torch.reshape(Q, new_shape), 1, 2)\n",
    "        K = torch.transpose(torch.reshape(K, new_shape), 1, 2)\n",
    "        V = torch.transpose(torch.reshape(V, new_shape), 1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, torch.transpose(K, -1, -2))/torch.sqrt(self.each_head_dim)\n",
    "        scores  = self.attn_dropout(self.softmax(scores))\n",
    "        \n",
    "        Z = torch.matmul(scores, V)\n",
    "        Z = torch.flatten(torch.transpose(Z, 1, 2), start_dim=2)\n",
    "        \n",
    "        out = self.linear(Z)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/model/utils/gelu.py\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper Section 3.4, last paragraph notice that BERT used the GELU instead of RELU\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=config.emb_dim, out_features=config.hidden_dim)\n",
    "        self.fc2 = nn.Linear(in_features=config.hidden_dim, out_features=config.emb_dim)\n",
    "        self.gelu = GELU()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=config.emb_dim, eps=config.eps)\n",
    "        self.fc_dropout = nn.Dropout(p = config.fc_drop_rate)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        out = self.fc1(X)\n",
    "        out = self.gelu(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out += X\n",
    "        out = self.layer_norm(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiHeadSelfAttentionLayer(config)\n",
    "        \n",
    "        self.feed_forward = FeedForwardLayer(config)\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        \n",
    "        out = self.self_attention(embeddings)\n",
    "        out = self.feed_forward(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super(BERTEncoder, self).__init__()\n",
    "        \n",
    "        self.enbedding_layer = EmbeddingLayer(config)\n",
    "        \n",
    "        self.encoders = nn.ModuleList([EncoderLayer(config) for i in range(config.num_encoders)])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        embeddings = self.enbedding_layer(X)\n",
    "        \n",
    "        for encoder in self.encoders:\n",
    "            \n",
    "            embeddings = encoder(embeddings)\n",
    "            \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "model = BERTEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/dhlee347/pytorchic-bert/blob/master/checkpoint.py\n",
    "\n",
    "\"\"\" Load a checkpoint file of pretrained transformer to a model in pytorch \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "#import ipdb\n",
    "#from models import *\n",
    "\n",
    "def load_param(checkpoint_file, conversion_table):\n",
    "    \"\"\"\n",
    "    Load parameters in pytorch model from checkpoint file according to conversion_table\n",
    "    checkpoint_file : pretrained checkpoint model file in tensorflow\n",
    "    conversion_table : { pytorch tensor in a model : checkpoint variable name }\n",
    "    \"\"\"\n",
    "    for pyt_param, tf_param_name in conversion_table.items():\n",
    "        tf_param = tf.train.load_variable(checkpoint_file, tf_param_name)\n",
    "\n",
    "        # for weight(kernel), we should do transpose\n",
    "        if tf_param_name.endswith('kernel'):\n",
    "            tf_param = np.transpose(tf_param)\n",
    "\n",
    "        assert pyt_param.size() == tf_param.shape, \\\n",
    "            'Dim Mismatch: %s vs %s ; %s' % \\\n",
    "                (tuple(pyt_param.size()), tf_param.shape, tf_param_name)\n",
    "        \n",
    "        # assign pytorch tensor from tensorflow param\n",
    "        pyt_param.data = torch.from_numpy(tf_param)\n",
    "\n",
    "\n",
    "def load_model(model, checkpoint_file):\n",
    "    \"\"\" Load the pytorch model from checkpoint file \"\"\"\n",
    "\n",
    "    # Embedding layer\n",
    "    e, p = model.enbedding_layer, 'bert/embeddings/'\n",
    "    load_param(checkpoint_file, {\n",
    "        e.token_embeddings.weight: p+\"word_embeddings\",\n",
    "        e.positional_embeddings.weight: p+\"position_embeddings\",\n",
    "        e.sentence_embeddings.weight: p+\"token_type_embeddings\",\n",
    "        e.layer_norm.weight:       p+\"LayerNorm/gamma\",\n",
    "        e.layer_norm.bias:        p+\"LayerNorm/beta\"\n",
    "    })\n",
    "\n",
    "    # Transformer blocks\n",
    "    for i in range(len(model.encoders)):\n",
    "        b, p = model.encoders[i], \"bert/encoder/layer_%d/\"%i\n",
    "        load_param(checkpoint_file, {\n",
    "            b.self_attention.queries.weight:   p+\"attention/self/query/kernel\",\n",
    "            b.self_attention.queries.bias:     p+\"attention/self/query/bias\",\n",
    "            b.self_attention.keys.weight:   p+\"attention/self/key/kernel\",\n",
    "            b.self_attention.keys.bias:     p+\"attention/self/key/bias\",\n",
    "            b.self_attention.values.weight:   p+\"attention/self/value/kernel\",\n",
    "            b.self_attention.values.bias:     p+\"attention/self/value/bias\",\n",
    "            b.self_attention.linear.weight:          p+\"attention/output/dense/kernel\",\n",
    "            b.self_attention.linear.bias:            p+\"attention/output/dense/bias\",\n",
    "            b.feed_forward.fc1.weight:      p+\"intermediate/dense/kernel\",\n",
    "            b.feed_forward.fc1.bias:        p+\"intermediate/dense/bias\",\n",
    "            b.feed_forward.fc2.weight:      p+\"output/dense/kernel\",\n",
    "            b.feed_forward.fc2.bias:        p+\"output/dense/bias\",\n",
    "            b.feed_forward.layer_norm.weight:          p+\"attention/output/LayerNorm/gamma\",\n",
    "            b.feed_forward.layer_norm.bias:           p+\"attention/output/LayerNorm/beta\",\n",
    "            b.feed_forward.layer_norm.weight:          p+\"output/LayerNorm/gamma\",\n",
    "            b.feed_forward.layer_norm.bias:           p+\"output/LayerNorm/beta\",\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"weights_uncased/bert_model.ckpt\"\n",
    "load_model(model, weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enbedding_layer.token_embeddings.weight\n",
      "enbedding_layer.sentence_embeddings.weight\n",
      "enbedding_layer.positional_embeddings.weight\n",
      "enbedding_layer.layer_norm.weight\n",
      "enbedding_layer.layer_norm.bias\n",
      "encoders.0.self_attention.queries.weight\n",
      "encoders.0.self_attention.queries.bias\n",
      "encoders.0.self_attention.keys.weight\n",
      "encoders.0.self_attention.keys.bias\n",
      "encoders.0.self_attention.values.weight\n",
      "encoders.0.self_attention.values.bias\n",
      "encoders.0.self_attention.linear.weight\n",
      "encoders.0.self_attention.linear.bias\n",
      "encoders.0.feed_forward.fc1.weight\n",
      "encoders.0.feed_forward.fc1.bias\n",
      "encoders.0.feed_forward.fc2.weight\n",
      "encoders.0.feed_forward.fc2.bias\n",
      "encoders.0.feed_forward.layer_norm.weight\n",
      "encoders.0.feed_forward.layer_norm.bias\n",
      "encoders.1.self_attention.queries.weight\n",
      "encoders.1.self_attention.queries.bias\n",
      "encoders.1.self_attention.keys.weight\n",
      "encoders.1.self_attention.keys.bias\n",
      "encoders.1.self_attention.values.weight\n",
      "encoders.1.self_attention.values.bias\n",
      "encoders.1.self_attention.linear.weight\n",
      "encoders.1.self_attention.linear.bias\n",
      "encoders.1.feed_forward.fc1.weight\n",
      "encoders.1.feed_forward.fc1.bias\n",
      "encoders.1.feed_forward.fc2.weight\n",
      "encoders.1.feed_forward.fc2.bias\n",
      "encoders.1.feed_forward.layer_norm.weight\n",
      "encoders.1.feed_forward.layer_norm.bias\n",
      "encoders.2.self_attention.queries.weight\n",
      "encoders.2.self_attention.queries.bias\n",
      "encoders.2.self_attention.keys.weight\n",
      "encoders.2.self_attention.keys.bias\n",
      "encoders.2.self_attention.values.weight\n",
      "encoders.2.self_attention.values.bias\n",
      "encoders.2.self_attention.linear.weight\n",
      "encoders.2.self_attention.linear.bias\n",
      "encoders.2.feed_forward.fc1.weight\n",
      "encoders.2.feed_forward.fc1.bias\n",
      "encoders.2.feed_forward.fc2.weight\n",
      "encoders.2.feed_forward.fc2.bias\n",
      "encoders.2.feed_forward.layer_norm.weight\n",
      "encoders.2.feed_forward.layer_norm.bias\n",
      "encoders.3.self_attention.queries.weight\n",
      "encoders.3.self_attention.queries.bias\n",
      "encoders.3.self_attention.keys.weight\n",
      "encoders.3.self_attention.keys.bias\n",
      "encoders.3.self_attention.values.weight\n",
      "encoders.3.self_attention.values.bias\n",
      "encoders.3.self_attention.linear.weight\n",
      "encoders.3.self_attention.linear.bias\n",
      "encoders.3.feed_forward.fc1.weight\n",
      "encoders.3.feed_forward.fc1.bias\n",
      "encoders.3.feed_forward.fc2.weight\n",
      "encoders.3.feed_forward.fc2.bias\n",
      "encoders.3.feed_forward.layer_norm.weight\n",
      "encoders.3.feed_forward.layer_norm.bias\n",
      "encoders.4.self_attention.queries.weight\n",
      "encoders.4.self_attention.queries.bias\n",
      "encoders.4.self_attention.keys.weight\n",
      "encoders.4.self_attention.keys.bias\n",
      "encoders.4.self_attention.values.weight\n",
      "encoders.4.self_attention.values.bias\n",
      "encoders.4.self_attention.linear.weight\n",
      "encoders.4.self_attention.linear.bias\n",
      "encoders.4.feed_forward.fc1.weight\n",
      "encoders.4.feed_forward.fc1.bias\n",
      "encoders.4.feed_forward.fc2.weight\n",
      "encoders.4.feed_forward.fc2.bias\n",
      "encoders.4.feed_forward.layer_norm.weight\n",
      "encoders.4.feed_forward.layer_norm.bias\n",
      "encoders.5.self_attention.queries.weight\n",
      "encoders.5.self_attention.queries.bias\n",
      "encoders.5.self_attention.keys.weight\n",
      "encoders.5.self_attention.keys.bias\n",
      "encoders.5.self_attention.values.weight\n",
      "encoders.5.self_attention.values.bias\n",
      "encoders.5.self_attention.linear.weight\n",
      "encoders.5.self_attention.linear.bias\n",
      "encoders.5.feed_forward.fc1.weight\n",
      "encoders.5.feed_forward.fc1.bias\n",
      "encoders.5.feed_forward.fc2.weight\n",
      "encoders.5.feed_forward.fc2.bias\n",
      "encoders.5.feed_forward.layer_norm.weight\n",
      "encoders.5.feed_forward.layer_norm.bias\n",
      "encoders.6.self_attention.queries.weight\n",
      "encoders.6.self_attention.queries.bias\n",
      "encoders.6.self_attention.keys.weight\n",
      "encoders.6.self_attention.keys.bias\n",
      "encoders.6.self_attention.values.weight\n",
      "encoders.6.self_attention.values.bias\n",
      "encoders.6.self_attention.linear.weight\n",
      "encoders.6.self_attention.linear.bias\n",
      "encoders.6.feed_forward.fc1.weight\n",
      "encoders.6.feed_forward.fc1.bias\n",
      "encoders.6.feed_forward.fc2.weight\n",
      "encoders.6.feed_forward.fc2.bias\n",
      "encoders.6.feed_forward.layer_norm.weight\n",
      "encoders.6.feed_forward.layer_norm.bias\n",
      "encoders.7.self_attention.queries.weight\n",
      "encoders.7.self_attention.queries.bias\n",
      "encoders.7.self_attention.keys.weight\n",
      "encoders.7.self_attention.keys.bias\n",
      "encoders.7.self_attention.values.weight\n",
      "encoders.7.self_attention.values.bias\n",
      "encoders.7.self_attention.linear.weight\n",
      "encoders.7.self_attention.linear.bias\n",
      "encoders.7.feed_forward.fc1.weight\n",
      "encoders.7.feed_forward.fc1.bias\n",
      "encoders.7.feed_forward.fc2.weight\n",
      "encoders.7.feed_forward.fc2.bias\n",
      "encoders.7.feed_forward.layer_norm.weight\n",
      "encoders.7.feed_forward.layer_norm.bias\n",
      "encoders.8.self_attention.queries.weight\n",
      "encoders.8.self_attention.queries.bias\n",
      "encoders.8.self_attention.keys.weight\n",
      "encoders.8.self_attention.keys.bias\n",
      "encoders.8.self_attention.values.weight\n",
      "encoders.8.self_attention.values.bias\n",
      "encoders.8.self_attention.linear.weight\n",
      "encoders.8.self_attention.linear.bias\n",
      "encoders.8.feed_forward.fc1.weight\n",
      "encoders.8.feed_forward.fc1.bias\n",
      "encoders.8.feed_forward.fc2.weight\n",
      "encoders.8.feed_forward.fc2.bias\n",
      "encoders.8.feed_forward.layer_norm.weight\n",
      "encoders.8.feed_forward.layer_norm.bias\n",
      "encoders.9.self_attention.queries.weight\n",
      "encoders.9.self_attention.queries.bias\n",
      "encoders.9.self_attention.keys.weight\n",
      "encoders.9.self_attention.keys.bias\n",
      "encoders.9.self_attention.values.weight\n",
      "encoders.9.self_attention.values.bias\n",
      "encoders.9.self_attention.linear.weight\n",
      "encoders.9.self_attention.linear.bias\n",
      "encoders.9.feed_forward.fc1.weight\n",
      "encoders.9.feed_forward.fc1.bias\n",
      "encoders.9.feed_forward.fc2.weight\n",
      "encoders.9.feed_forward.fc2.bias\n",
      "encoders.9.feed_forward.layer_norm.weight\n",
      "encoders.9.feed_forward.layer_norm.bias\n",
      "encoders.10.self_attention.queries.weight\n",
      "encoders.10.self_attention.queries.bias\n",
      "encoders.10.self_attention.keys.weight\n",
      "encoders.10.self_attention.keys.bias\n",
      "encoders.10.self_attention.values.weight\n",
      "encoders.10.self_attention.values.bias\n",
      "encoders.10.self_attention.linear.weight\n",
      "encoders.10.self_attention.linear.bias\n",
      "encoders.10.feed_forward.fc1.weight\n",
      "encoders.10.feed_forward.fc1.bias\n",
      "encoders.10.feed_forward.fc2.weight\n",
      "encoders.10.feed_forward.fc2.bias\n",
      "encoders.10.feed_forward.layer_norm.weight\n",
      "encoders.10.feed_forward.layer_norm.bias\n",
      "encoders.11.self_attention.queries.weight\n",
      "encoders.11.self_attention.queries.bias\n",
      "encoders.11.self_attention.keys.weight\n",
      "encoders.11.self_attention.keys.bias\n",
      "encoders.11.self_attention.values.weight\n",
      "encoders.11.self_attention.values.bias\n",
      "encoders.11.self_attention.linear.weight\n",
      "encoders.11.self_attention.linear.bias\n",
      "encoders.11.feed_forward.fc1.weight\n",
      "encoders.11.feed_forward.fc1.bias\n",
      "encoders.11.feed_forward.fc2.weight\n",
      "encoders.11.feed_forward.fc2.bias\n",
      "encoders.11.feed_forward.layer_norm.weight\n",
      "encoders.11.feed_forward.layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "for params in model.named_parameters():\n",
    "    print(params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
