{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertForTokenClassification, BertModel\n",
    "import torch, tqdm, json\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "# from scorer impbort fever_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(fname, train=True):\n",
    "    X = []\n",
    "    mask = []\n",
    "    token_type_ids = []\n",
    "    y = []\n",
    "\n",
    "    label_dict = {}\n",
    "    label_dict['UNK'] = -1\n",
    "    label_dict['NOT ENOUGH INFO'] = 0\n",
    "    label_dict['SUPPORTS'] = 1\n",
    "    label_dict['REFUTES'] = 2\n",
    "    claim_ids = []\n",
    "\n",
    "    predicted_evidence = []\n",
    "    f = open(fname, encoding='utf8')\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        claim_ids.append(line['id'])\n",
    "        predicted_evidence.append([line['doc'], line['sid']])\n",
    "\n",
    "        emb = tokenizer.encode_plus(line['claim'], line[\"sentence\"], pad_to_max_length=True)\n",
    "        input_ids, sent_ids, m = emb['input_ids'], emb['token_type_ids'], emb['attention_mask']\n",
    "        \n",
    "        X.append(input_ids[:128])\n",
    "        mask.append(m[:128])\n",
    "        token_type_ids.append(sent_ids[:128])\n",
    "        \n",
    "        y.append(label_dict[line['label']])\n",
    "    f.close()\n",
    "        \n",
    "    return torch.LongTensor(X), torch.LongTensor(y), torch.LongTensor(mask), torch.LongTensor(token_type_ids), claim_ids, predicted_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NN-NLP-Project-Data/train_sent_results.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-0fbada72cd00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_evidence_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NN-NLP-Project-Data/train_sent_results.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_evidence_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NN-NLP-Project-Data/dev_sent_results.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# X_test, y_test, mask_test, ids_test, predicted_evidence_test = process_data(\"NN-NLP-Project-Data/test_sent_results.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-1ecc886fcc06>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(fname, train)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpredicted_evidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NN-NLP-Project-Data/train_sent_results.txt'"
     ]
    }
   ],
   "source": [
    "X_train, y_train, mask_train, token_type_ids_train, ids_train, predicted_evidence_train = process_data(\"NN-NLP-Project-Data/train_sent_results.txt\")\n",
    "X_dev, y_dev, mask_dev, token_type_ids_dev, ids_dev, predicted_evidence_dev = process_data(\"NN-NLP-Project-Data/dev_sent_results.txt\")\n",
    "# X_test, y_test, mask_test, ids_test, predicted_evidence_test = process_data(\"NN-NLP-Project-Data/test_sent_results.txt\")\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train, mask_train, token_type_ids)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32, num_workers=8)\n",
    "\n",
    "dev_dataset = TensorDataset(X_dev, y_dev, mask_dev, token_type_ids)\n",
    "dev_loader = DataLoader(dev_dataset, shuffle=False, batch_size=32, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSelfAttn(nn.Module):\n",
    "    \"\"\"Self attention over a sequence:\n",
    "    * o_i = softmax(Wx_i) for x_i in X.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, drop_rate=0.1):\n",
    "        super(LinearSelfAttn, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "        self.dropout = torch.nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        x = self.dropout(x)\n",
    "        x_flat = x.contiguous().view(-1, x.size(-1))\n",
    "        scores = self.linear(x_flat).view(x.size(0), x.size(1))\n",
    "        scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        alpha = F.softmax(scores, 1)\n",
    "        return alpha.unsqueeze(1).bmm(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearFlatSim(nn.Module):\n",
    "    \"\"\"A bilinear attention layer over a sequence X w.r.t y:\n",
    "    * o_i = x_i'Wy for x_i in X.\n",
    "    \"\"\"\n",
    "    def __init__(self, x_size, y_size, drop_rate=0.1):\n",
    "        super(BilinearFlatSim, self).__init__()\n",
    "  \n",
    "        self.linear = nn.Linear(y_size, x_size)\n",
    "        self.dropout = torch.nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x, y, x_mask):\n",
    "        \"\"\"\n",
    "        x = batch * len * h1\n",
    "        y = batch * h2\n",
    "        x_mask = batch * len\n",
    "        \"\"\"\n",
    "        x = self.dropout(x)\n",
    "        y = self.dropout(y)\n",
    "\n",
    "        Wy = self.linear(y)\n",
    "        xWy = x.bmm(Wy.unsqueeze(2)).squeeze(2)\n",
    "        xWy.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        return xWy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, x_size, y_size, drop_rate=0.1):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(drop_rate)\n",
    "        self.proj = nn.Linear(x_size * 4, y_size)\n",
    "\n",
    "    def forward(self, x1, x2, mask=None):\n",
    "        x = torch.cat([x1, x2, (x1 - x2).abs(), x1 * x2], 1)\n",
    "        x = self.dropout(x)\n",
    "        scores = self.proj(x)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(new_data, dropout_p=0.0, is_training=False):\n",
    "    if not is_training: dropout_p = 0.0\n",
    "    new_data = (1-dropout_p) * (new_data.zero_() + 1)\n",
    "    for i in range(new_data.size(0)):\n",
    "        one = random.randint(0, new_data.size(1)-1)\n",
    "        new_data[i][one] = 1\n",
    "    mask = 1.0/(1 - dropout_p) * torch.bernoulli(new_data)\n",
    "    mask.requires_grad = False\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSAN(nn.Module):\n",
    "    \"\"\"BERT model with SAN for entailment.\n",
    "    \"\"\"\n",
    "    def __init__(self, K, x_size, h_size, drop_rate=0.0, num_labels=3):\n",
    "        super(BertSAN, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.query_wsum = LinearSelfAttn(x_size, drop_rate=0.1)\n",
    "        self.attn = BilinearFlatSim(x_size, h_size)\n",
    "        self.rnn = torch.nn.GRUCell(input_size=768, hidden_size=768)\n",
    "\n",
    "        self.K = K\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(drop_rate)\n",
    "        self.classifier = Classifier(x_size, num_labels)\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.zeros(1, 1), requires_grad=False)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, labels=None):\n",
    "        \n",
    "        hidden_states, pooled_output = self.bert(input_ids)\n",
    "        \n",
    "        x_mask = torch.BoolTensor((token_type_ids==0) + (attention_mask==0))\n",
    "        h_mask = torch.BoolTensor((token_type_ids==1) + (attention_mask==0))\n",
    "        \n",
    "        h0 = x = hidden_states\n",
    "        \n",
    "        h0 = self.query_wsum(h0, h_mask)\n",
    "        scores_list = []\n",
    "\n",
    "        for turn in range(self.K):\n",
    "          att_scores = self.attn(x, h0, x_mask)\n",
    "          x_sum = torch.bmm(F.softmax(att_scores, 1).unsqueeze(1), x).squeeze(1)\n",
    "          scores = self.classifier(x_sum, h0)\n",
    "          scores_list.append(scores)\n",
    "\n",
    "          h0 = self.dropout(h0)\n",
    "          h0 = self.rnn(x_sum, h0)\n",
    "\n",
    "        mask = generate_mask(self.alpha.data.new(x.size(0), self.K), 0.1, self.training)\n",
    "        mask = [m.contiguous() for m in torch.unbind(mask, 1)]\n",
    "        tmp_scores_list = [mask[idx].view(x.size(0), 1).expand_as(inp) * F.softmax(inp, 1) for idx, inp in enumerate(scores_list)]\n",
    "        scores = torch.stack(tmp_scores_list, 2)\n",
    "        scores = torch.mean(scores, 2)\n",
    "        scores = torch.log(scores)\n",
    "        \n",
    "#         scores = scores_list[-1]\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_ip = torch.zeros(size=(1, 512), dtype=torch.long)\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# emb = tokenizer.encode(concat, add_special_tokens=True)[:128]\n",
    "\n",
    "# dic = tokenizer.encode_plus(claim, hyp, pad_to_max_length=True)\n",
    "# print(dic['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertSAN(10, 768, 768)\n",
    "\n",
    "claim = \"The world is big\"\n",
    "hyp = \"The world is big\"\n",
    "\n",
    "emb = tokenizer.encode_plus(claim, hyp, pad_to_max_length=True)\n",
    "input_ids, sent_ids, mask = torch.LongTensor(emb['input_ids'][:128]).unsqueeze(0), torch.LongTensor(emb['token_type_ids'][:128]).unsqueeze(0), torch.LongTensor(emb['attention_mask'][:128]).unsqueeze(0)\n",
    "\n",
    "out = model(input_ids, sent_ids, mask)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5544, -1.2000, -0.9772]], grad_fn=<LogBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
