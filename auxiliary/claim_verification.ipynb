{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V-d_JcecYo5Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, tqdm, time, json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDr7YWkrYo5W"
   },
   "outputs": [],
   "source": [
    "from tokenization import FullTokenizer\n",
    "from Bert import *\n",
    "from scorer import fever_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjbIBnANYo5a"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JGUmHMo1Yo5c"
   },
   "outputs": [],
   "source": [
    "weights_path = \"/content/drive/My Drive/NN-NLP-Project-Data/uncased_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "vocab_file = \"/content/drive/My Drive/NN-NLP-Project-Data/uncased_L-12_H-768_A-12/vocab.txt\"\n",
    "model_name = \"ClaimVerification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jErcqGykYo5g"
   },
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, tok_ip, sent_ip, pos_ip, masks, y):\n",
    "        self.tok_ip = tok_ip\n",
    "        self.sent_ip = sent_ip\n",
    "        self.pos_ip = pos_ip\n",
    "        self.masks = masks\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.tok_ip[index], self.sent_ip[index], self.pos_ip[index], self.masks[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_5yFaW0wYo5j"
   },
   "outputs": [],
   "source": [
    "class ClaimVerification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.enbedding_layer = EmbeddingLayer(config)\n",
    "        self.encoders = nn.ModuleList([EncoderLayer(config) for i in range(config.num_encoders)])\n",
    "        self.output = nn.Linear(config.emb_dim, 3)\n",
    "        \n",
    "    def forward(self, token_ip, sent_ip, pos_ip, mask=None):\n",
    "        embeddings = self.enbedding_layer(token_ip, sent_ip, pos_ip)\n",
    "        for encoder in self.encoders:\n",
    "            embeddings = encoder(embeddings, mask)\n",
    "        out = self.output(embeddings[:, 0])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJX86P3HYo5o"
   },
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    label_dict = {}\n",
    "    label_dict['UNK'] = -1\n",
    "    label_dict['NOT ENOUGH INFO'] = 0\n",
    "    label_dict['SUPPORTS'] = 1\n",
    "    label_dict['REFUTES'] = 2\n",
    "    f = open(fname, encoding='utf8')\n",
    "    data = []\n",
    "    claim_ids = []\n",
    "    labels = []\n",
    "    predicted_evidence = []\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        sentence = [\"[CLS]\" + line['claim'] + \"[SEP]\", line['sentence'] + \"[SEP]\"]\n",
    "        label = label_dict[line['label']]\n",
    "        data.append(sentence)\n",
    "        labels.append(label)\n",
    "        claim_ids.append(line['id'])\n",
    "        predicted_evidence.append([line['doc'], line['sid']])\n",
    "    f.close()\n",
    "    \n",
    "    return data, labels, claim_ids, predicted_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pT_xf10kYo5r"
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    tokenizer = FullTokenizer(vocab_file)\n",
    "    tok_ip = np.zeros((len(data), 128), dtype=\"int32\")\n",
    "    sent_ip = np.zeros((len(data), 128), dtype=\"int8\")\n",
    "    pos_ip = np.zeros((len(data), 128), dtype=\"int8\")\n",
    "    masks = np.zeros((len(data), 128), dtype=\"int8\")\n",
    "    \n",
    "    for pos, text in tqdm.tqdm_notebook(enumerate(data)):\n",
    "        tok0 = tokenizer.tokenize(text[0])\n",
    "        tok1 = tokenizer.tokenize(text[1])\n",
    "        tok = tok0 + tok1\n",
    "        if len(tok) > 128:\n",
    "            tok = tok[:127] + [\"[SEP]\"]\n",
    "        pad_len = 128-len(tok)\n",
    "        tok_len = len(tok)\n",
    "        tok0_len = len(tok0)\n",
    "        tok = tokenizer.convert_tokens_to_ids(tok) + [0]*pad_len\n",
    "        pos_val = range(128)\n",
    "        sent = [0]*tok0_len + [1]*(tok_len-tok0_len) + [0]*pad_len\n",
    "        mask = [1]*tok_len + [0]*pad_len\n",
    "        \n",
    "        tok_ip[pos] = tok\n",
    "        pos_ip[pos] = pos_val\n",
    "        masks[pos] = mask\n",
    "        \n",
    "    masks = masks[:, None, None, :]\n",
    "    return tok_ip, sent_ip, pos_ip, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVY1bbOxYo5u"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"train/train-tok.npy\"):\n",
    "    data, labels, ids, predicted_evidence = load_data(\"train-data.jsonl\")\n",
    "    tok_ip, sent_ip, pos_ip, masks = preprocess(data)\n",
    "    labels = np.array(labels)\n",
    "    os.mkdir(\"train\")\n",
    "    np.save(\"train/train-tok.npy\", tok_ip)\n",
    "    np.save(\"train/train-sent.npy\", sent_ip)\n",
    "    np.save(\"train/train-sent.npy\", pos_ip)\n",
    "    np.save(\"train/train-masks.npy\", masks)\n",
    "    np.save(\"train/train-labels.npy\", labels)\n",
    "else:\n",
    "    data, labels, ids, predicted_evidence = load_data(\"train-data.jsonl\")\n",
    "    tok_ip = np.load(\"train/train-tok.npy\")\n",
    "    sent_ip = np.load(\"train/train-sent.npy\")\n",
    "    pos_ip = np.load(\"train/train-sent.npy\")\n",
    "    masks = np.load(\"train/train-masks.npy\")\n",
    "    labels = np.load(\"train/train-labels.npy\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DF1MH413Yo5w"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"dev/dev-tok.npy\"):\n",
    "    data_dev, labels_dev, ids_dev, predicted_evidence_dev = load_data(\"dev-data.jsonl\")\n",
    "    tok_ip_dev, sent_ip_dev, pos_ip_dev, masks_dev = preprocess(data_dev)\n",
    "    labels_dev = np.array(labels_dev)\n",
    "    os.mkdir(\"dev\")\n",
    "    np.save(\"dev/dev-tok.npy\", tok_ip_dev)\n",
    "    np.save(\"dev/dev-sent.npy\", sent_ip_dev)\n",
    "    np.save(\"dev/dev-pos.npy\", pos_ip_dev)\n",
    "    np.save(\"dev/dev-masks.npy\", masks_dev)\n",
    "    np.save(\"dev/dev-labels.npy\", labels_dev)\n",
    "else:\n",
    "    data_dev, labels_dev, ids_dev, predicted_evidence_dev = load_data(\"dev-data.jsonl\")\n",
    "    tok_ip_dev = np.load(\"dev/dev-tok.npy\")\n",
    "    sent_ip_dev = np.load(\"dev/dev-sent.npy\")\n",
    "    pos_ip_dev = np.load(\"dev/dev-pos.npy\")\n",
    "    masks_dev = np.load(\"dev/dev-masks.npy\")\n",
    "    labels_dev = np.load(\"dev/dev-labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qm4CXSNdYo51"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"test/test-tok.npy\"):\n",
    "    data_test, labels_test, ids_test, predicted_evidence_test = load_data(\"test-data.jsonl\")\n",
    "    tok_ip_test, sent_ip_test, pos_ip_test, masks_test = preprocess(data_test)\n",
    "    labels_test = np.array(labels_test)\n",
    "    os.mkdir(\"test\")\n",
    "    np.save(\"test/test-tok.npy\", tok_ip_test)\n",
    "    np.save(\"test/test-sent.npy\", sent_ip_test)\n",
    "    np.save(\"test/test-pos.npy\", pos_ip_test)\n",
    "    np.save(\"test/test-masks.npy\", masks_test)\n",
    "    np.save(\"test/test-labels.npy\", labels_test)\n",
    "else:\n",
    "    data_test, labels_test, ids_test, predicted_evidence_test = load_data(\"test-data.jsonl\")\n",
    "    tok_ip_test = np.load(\"test/test-tok.npy\")\n",
    "    sent_ip_test = np.load(\"test/test-sent.npy\")\n",
    "    pos_ip_test = np.load(\"test/test-pos.npy\")\n",
    "    masks_test = np.load(\"test/test-masks.npy\")\n",
    "    labels_test = np.load(\"test/test-labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zvoM7U8DYo53"
   },
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    loss_epoch = 0\n",
    "    for tok_ip, sent_ip, pos_ip, masks, y in tqdm.tqdm_notebook(loader):\n",
    "        optimizer.zero_grad()\n",
    "        tok_ip = tok_ip.type(torch.LongTensor).to(device)\n",
    "        sent_ip = sent_ip.type(torch.LongTensor).to(device)\n",
    "        pos_ip = pos_ip.type(torch.LongTensor).to(device)\n",
    "        masks = masks.type(torch.FloatTensor).to(device)\n",
    "        y = y.to(device)\n",
    "        O = model(tok_ip, sent_ip, pos_ip, masks)\n",
    "        loss = criterion(O, y)\n",
    "        loss_epoch += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print (\"Loss:\", loss_epoch/len(loader))\n",
    "    \n",
    "    return loss_epoch/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFvkgobG9ice"
   },
   "outputs": [],
   "source": [
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    for tok_ip, sent_ip, pos_ip, masks, y in tqdm.tqdm_notebook(loader):\n",
    "        optimizer.zero_grad()\n",
    "        tok_ip = tok_ip.to(device)\n",
    "        sent_ip = sent_ip.to(device)\n",
    "        pos_ip = pos_ip.to(device)\n",
    "        masks = masks.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(tok_ip, sent_ip, pos_ip, masks)\n",
    "        outputs.extend(output.detach().cpu().argmax(dim=1).numpy())\n",
    "\n",
    "    return np.asarray(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-kfuJ8gBydU"
   },
   "outputs": [],
   "source": [
    "# Merge predictions for each claim\n",
    "def merge_preds(preds, ids, predicted_evidence):\n",
    "    merged_preds = []\n",
    "    merged_evidence = []\n",
    "    cur_id = ids[0]\n",
    "    # Indices represent NEI, Supports, Refutes\n",
    "    stats = [0, 0, 0]\n",
    "    evidence_line = []\n",
    "    stats[preds[0]] += 1\n",
    "    for i in range(1,len(ids)):\n",
    "        if ids[i] == cur_id:\n",
    "            stats[preds[i]] += 1\n",
    "            evidence_line.append(predicted_evidence[i])\n",
    "        else:\n",
    "            # Label Assignment according to rules mentioned in paper\n",
    "            if stats[1] > 0:\n",
    "                merged_preds.append(\"SUPPORTS\")\n",
    "            elif stats[2] > 0 and stats[1] == 0:\n",
    "                merged_preds.append(\"REFUTES\")\n",
    "            elif stats[1] == 0 and stats[2] == 0:\n",
    "                merged_preds.append(\"NOT ENOUGH INFO\")\n",
    "            stats = [0, 0, 0]\n",
    "            cur_id = ids[i]\n",
    "            stats[preds[i]] += 1\n",
    "            merged_evidence.append(evidence_line)\n",
    "            evidence_line = []\n",
    "    if stats[1] > 0:\n",
    "        merged_preds.append(\"SUPPORTS\")\n",
    "    elif stats[2] > 0 and stats[1] == 0:\n",
    "        merged_preds.append(\"REFUTES\")\n",
    "    elif stats[1] == 0 and stats[2] == 0:\n",
    "        merged_preds.append(\"NOT ENOUGH INFO\")\n",
    "    merged_evidence.append(evidence_line)\n",
    "    return merged_preds, merged_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dqxp92G4J0Au"
   },
   "outputs": [],
   "source": [
    "# Make final json with id, label, predicted_label, evidence and predicted_evidence\n",
    "def format_output(in_path, out_path, preds, evidence, dev=True):\n",
    "    outputs = []\n",
    "    with open(in_path, 'r', encoding='utf8') as f:\n",
    "        i = 0\n",
    "        for line in f.readlines():\n",
    "            output_obj = {}\n",
    "            input_obj = json.loads(line.strip())\n",
    "            output_obj['id'] = input_obj['id']\n",
    "            if dev:\n",
    "                output_obj['label'] = input_obj['label']\n",
    "            output_obj['predicted_label'] = preds[i]\n",
    "            output_obj['predicted_evidence'] = evidence[i]\n",
    "            if dev:\n",
    "                output_obj['evidence'] = input_obj['evidence']\n",
    "            i += 1\n",
    "            outputs.append(output_obj)\n",
    "\n",
    "    # Calculate Fever score for dev set\n",
    "    if dev:\n",
    "        print('Dev Set Results')\n",
    "        fever_sc, label_accuracy, precision, recall, f1 = fever_score(outputs)\n",
    "        print('Fever Score: ',fever_sc)     \n",
    "        print('Label Accuracy: ',label_accuracy)   \n",
    "        print('Precision: ',precision)       \n",
    "        print('Recall: ',recall)      \n",
    "        print('F1 Score: ',f1)    \n",
    "\n",
    "    # Write final predictions to file\n",
    "    with open(out_path, 'w', encoding='utf8') as f:\n",
    "        for line in outputs:\n",
    "            json.dump(line, f)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GYPYNg0SYo56"
   },
   "outputs": [],
   "source": [
    "train_dataset = SentenceDataset(tok_ip, sent_ip, pos_ip, masks, labels)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUaRD_EBYo59"
   },
   "outputs": [],
   "source": [
    "dev_dataset = SentenceDataset(tok_ip_dev, sent_ip_dev, pos_ip_dev, masks_dev, labels_dev)\n",
    "dev_loader = DataLoader(dev_dataset, shuffle=False, batch_size=32, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LuPBNdHmYo6A"
   },
   "outputs": [],
   "source": [
    "test_dataset = SentenceDataset(tok_ip_test, sent_ip_test, pos_ip_test, masks_test, labels_test)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=32, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HygPTFOXYo6D"
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "model = ClaimVerification(config)\n",
    "load_model(model, weights_path)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLViX1G7Yo6G"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "for i in range(1):\n",
    "    x = train(model, dev_loader, criterion, optimizer)\n",
    "    torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DpvmrSDajQmd"
   },
   "outputs": [],
   "source": [
    "# Dev Set\n",
    "preds = test(model, dev_loader)\n",
    "merged_preds, merged_evidence = merge_preds(preds, ids_dev, predicted_evidence_dev)\n",
    "format_output('dev.jsonl', 'dev_results.txt', merged_preds, merged_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vq-EeK5YkF29"
   },
   "outputs": [],
   "source": [
    "# Test Set\n",
    "preds = test(model, test_loader)\n",
    "merged_preds, merged_evidence = merge_preds(preds, ids_dev, predicted_evidence_dev)\n",
    "format_output('test.jsonl', 'test_results.txt', merged\n",
    "_preds, merged_evidence)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "claim_verification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
