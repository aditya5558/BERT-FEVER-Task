{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, tqdm, time, json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derik/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/derik/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/derik/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/derik/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/derik/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/derik/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tokenization import FullTokenizer\n",
    "from Bert import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"uncased_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "vocab_file = \"uncased_L-12_H-768_A-12/vocab.txt\"\n",
    "model_name = \"SentenceRetrieval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, tok_ip, sent_ip, pos_ip, masks, y):\n",
    "        self.tok_ip = tok_ip\n",
    "        self.sent_ip = sent_ip\n",
    "        self.pos_ip = pos_ip\n",
    "        self.masks = masks\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.tok_ip[index], self.sent_ip[index], self.pos_ip[index], self.masks[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceRetrieval(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.enbedding_layer = EmbeddingLayer(config)\n",
    "        self.encoders = nn.ModuleList([EncoderLayer(config) for i in range(config.num_encoders)])\n",
    "        self.output = nn.Linear(config.emb_dim, 2)\n",
    "        \n",
    "    def forward(self, token_ip, sent_ip, pos_ip, mask=None):\n",
    "        embeddings = self.enbedding_layer(token_ip, sent_ip, pos_ip)\n",
    "        for encoder in self.encoders:\n",
    "            embeddings = encoder(embeddings, mask)\n",
    "        out = self.output(embeddings[:, 0])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(fname):\n",
    "#     f = open(fname, encoding='utf8')\n",
    "#     data = []\n",
    "#     labels = []\n",
    "#     for line in f:\n",
    "#         line = json.loads(line)\n",
    "#         sentence = [\"[CLS]\" + line['claim'] + \"[SEP]\", line['sentence'] + \"[SEP]\"]\n",
    "#         label = line['is_evidence']\n",
    "#         data.append(sentence)\n",
    "#         labels.append(label)\n",
    "#     f.close()\n",
    "    \n",
    "#     return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    f = open(fname, encoding='utf8')\n",
    "    data = []\n",
    "    claim_ids = []\n",
    "    labels = []\n",
    "    predicted_evidence = []\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        sentence = [\"[CLS]\" + line['claim'] + \"[SEP]\", line['doc'] + \" \" + line['sentence'] + \"[SEP]\"]\n",
    "        label = line['is_evidence']\n",
    "        data.append(sentence)\n",
    "        labels.append(label)\n",
    "        claim_ids.append(line['id'])\n",
    "        predicted_evidence.append([line['doc'], line['sid'], line['claim'], line['sentence'], line['label']])\n",
    "    f.close()\n",
    "    \n",
    "    return data, labels, np.array(claim_ids), predicted_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    tokenizer = FullTokenizer(vocab_file)\n",
    "    tok_ip = np.zeros((len(data), 512), dtype=\"int32\")\n",
    "    sent_ip = np.zeros((len(data), 512), dtype=\"int8\")\n",
    "    pos_ip = np.zeros((len(data), 512), dtype=\"int8\")\n",
    "    masks = np.zeros((len(data), 512), dtype=\"int8\")\n",
    "    \n",
    "    for pos, text in tqdm.tqdm_notebook(enumerate(data)):\n",
    "        tok0 = tokenizer.tokenize(text[0])\n",
    "        tok1 = tokenizer.tokenize(text[1])\n",
    "        tok = tok0 + tok1\n",
    "        if len(tok) > 512:\n",
    "            tok = tok[:511] + [\"[SEP]\"]\n",
    "        pad_len = 512-len(tok)\n",
    "        tok_len = len(tok)\n",
    "        tok0_len = len(tok0)\n",
    "        tok = tokenizer.convert_tokens_to_ids(tok) + [0]*pad_len\n",
    "        pos_val = range(512)\n",
    "        sent = [0]*tok0_len + [1]*(tok_len-tok0_len) + [0]*pad_len\n",
    "        mask = [1]*tok_len + [0]*pad_len\n",
    "        \n",
    "        tok_ip[pos] = tok\n",
    "        pos_ip[pos] = pos_val\n",
    "        masks[pos] = mask\n",
    "        \n",
    "    masks = masks[:, None, None, :]\n",
    "    return tok_ip, sent_ip, pos_ip, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0940f28eb5d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train/train-tok.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train-data.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtok_ip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_ip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_ip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-feb848938fea>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"[CLS]\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'claim'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"[SEP]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"[SEP]\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_train, labels_train, ids_train, predicted_evidence_train = load_data(\"train-data.jsonl\")\n",
    "\n",
    "if not os.path.exists(\"train/train-tok.npy\"):\n",
    "    tok_ip, sent_ip, pos_ip, masks = preprocess(data_train)\n",
    "    labels = np.array(labels_train)\n",
    "    os.mkdir(\"train\")\n",
    "    np.save(\"train/train-tok.npy\", tok_ip)\n",
    "    np.save(\"train/train-sent.npy\", sent_ip)\n",
    "    np.save(\"train/train-sent.npy\", pos_ip)\n",
    "    np.save(\"train/train-masks.npy\", masks)\n",
    "    np.save(\"train/train-labels.npy\", labels)\n",
    "else:\n",
    "    tok_ip = np.load(\"train/train-tok.npy\")\n",
    "    sent_ip = np.load(\"train/train-sent.npy\")\n",
    "    pos_ip = np.load(\"train/train-sent.npy\")\n",
    "    masks = np.load(\"train/train-masks.npy\")\n",
    "    labels = np.load(\"train/train-labels.npy\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/abhinavk/Desktop/fever/tokenization.py:123: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c817d54d8cff4fa5a7d4efecc3c8d32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-da5d9870fba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabels_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dev/dev-tok.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok_ip_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dev/dev-sent.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_ip_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dev/dev-pos.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_ip_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[0;32m--> 542\u001b[0;31m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             for chunk in numpy.nditer(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_dev, labels_dev, ids_dev, predicted_evidence_dev = load_data(\"dev-data.jsonl\")\n",
    "\n",
    "if not os.path.exists(\"dev/dev-tok.npy\"):\n",
    "    tok_ip_dev, sent_ip_dev, pos_ip_dev, masks_dev = preprocess(data_dev)\n",
    "    labels_dev = np.array(labels_dev)\n",
    "    os.mkdir(\"dev\")\n",
    "    np.save(\"dev/dev-tok.npy\", tok_ip_dev)\n",
    "    np.save(\"dev/dev-sent.npy\", sent_ip_dev)\n",
    "    np.save(\"dev/dev-pos.npy\", pos_ip_dev)\n",
    "    np.save(\"dev/dev-masks.npy\", masks_dev)\n",
    "    np.save(\"dev/dev-labels.npy\", labels_dev)\n",
    "else:\n",
    "    tok_ip_dev = np.load(\"dev/dev-tok.npy\")\n",
    "    sent_ip_dev = np.load(\"dev/dev-sent.npy\")\n",
    "    pos_ip_dev = np.load(\"dev/dev-pos.npy\")\n",
    "    masks_dev = np.load(\"dev/dev-masks.npy\")\n",
    "    labels_dev = np.load(\"dev/dev-labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test, labels_test, ids_test, predicted_evidence_test = load_data(\"test-data.jsonl\")\n",
    "\n",
    "if not os.path.exists(\"test/test-tok.npy\"):\n",
    "    tok_ip_test, sent_ip_test, pos_ip_test, masks_test = preprocess(data_test)\n",
    "    labels_test = np.array(labels_test)\n",
    "    os.mkdir(\"test\")\n",
    "    np.save(\"test/test-tok.npy\", tok_ip_test)\n",
    "    np.save(\"test/test-sent.npy\", sent_ip_test)\n",
    "    np.save(\"test/test-pos.npy\", pos_ip_test)\n",
    "    np.save(\"test/test-masks.npy\", masks_test)\n",
    "    np.save(\"test/test-labels.npy\", labels_test)\n",
    "else:\n",
    "    tok_ip_test = np.load(\"test/test-tok.npy\")\n",
    "    sent_ip_test = np.load(\"test/test-sent.npy\")\n",
    "    pos_ip_test = np.load(\"test/test-pos.npy\")\n",
    "    masks_test = np.load(\"test/test-masks.npy\")\n",
    "    labels_test = np.load(\"test/test-labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    loss_epoch = 0\n",
    "    for tok_ip, sent_ip, pos_ip, masks, y in tqdm.tqdm_notebook(loader):\n",
    "        optimizer.zero_grad()\n",
    "        tok_ip = tok_ip.type(torch.LongTensor).to(device)\n",
    "        sent_ip = sent_ip.type(torch.LongTensor).to(device)\n",
    "        pos_ip = pos_ip.type(torch.LongTensor).to(device)\n",
    "        masks = masks.type(torch.FloatTensor).to(device)\n",
    "        y = y.to(device)\n",
    "        O = model(tok_ip, sent_ip, pos_ip, masks)\n",
    "        loss = criterion(O, y)\n",
    "        loss_epoch += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print (\"Loss:\", loss_epoch/len(loader))\n",
    "    \n",
    "    return loss_epoch/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    scores = []\n",
    "    for tok_ip, sent_ip, pos_ip, masks, y in tqdm.tqdm_notebook(loader):\n",
    "        optimizer.zero_grad()\n",
    "        tok_ip = tok_ip.to(device)\n",
    "        sent_ip = sent_ip.to(device)\n",
    "        pos_ip = pos_ip.to(device)\n",
    "        masks = masks.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(tok_ip, sent_ip, pos_ip, masks)\n",
    "        \n",
    "        scores.extend(output.detach().cpu().numpy()[1])\n",
    "        outputs.extend(output.detach().cpu().argmax(dim=1).numpy())\n",
    "\n",
    "    return np.asarray(outputs), np.asarray(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge predictions for each claim\n",
    "def get_top_5(preds, scores, ids, predicted_evidence):\n",
    "    \n",
    "    evidence_map = {}\n",
    "    top_5_map = {}\n",
    "    \n",
    "    for i in range(len(ids)):\n",
    "        if preds[i] != 1:\n",
    "            continue\n",
    "        if ids[i] not in evidence_map.keys():\n",
    "            evidence_map[ids[i]] = []\n",
    "        evidence_map[ids[i]].append((scores[i], predicted_evidence))\n",
    "        \n",
    "    for id, sents in evidence_map.items():\n",
    "        top_5_sents = sorted(sents, key=lambda x: x[0], reverse=True)[:5]\n",
    "        top_5_map[id] = top_5_sents\n",
    "    \n",
    "    return top_5_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final json with id, label, predicted_label, evidence and predicted_evidence\n",
    "def format_output(out_path, top_5_map):\n",
    "    \n",
    "    outputs = []\n",
    "    for id, sents in top_5_map.items():\n",
    "        \n",
    "        for sent, meta in sents:\n",
    "            output_obj = {}\n",
    "            output_obj['id'] = id\n",
    "            output_obj['claim'] =  meta[2]\n",
    "            output_obj['label'] = meta[4]\n",
    "            output_obj['doc'] = meta[0]\n",
    "            output_obj['sid'] = meta[1]\n",
    "            output_obj['sentence'] = meta[3]\n",
    "\n",
    "    # Write final predictions to file\n",
    "    with open(out_path, 'w', encoding='utf8') as f:\n",
    "        for line in outputs:\n",
    "            json.dump(line, f)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentenceDataset(tok_ip, sent_ip, pos_ip, masks, labels)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=16, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = SentenceDataset(tok_ip_dev, sent_ip_dev, pos_ip_dev, masks_dev, labels_dev)\n",
    "dev_loader = DataLoader(dev_dataset, shuffle=False, batch_size=16, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SentenceDataset(tok_ip_test, sent_ip_test, pos_ip_test, masks_test, labels_test)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=16, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "model = SentenceRetrieval(config)\n",
    "load_model(model, weights_path)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    x = train(model, dev_loader, criterion, optimizer)\n",
    "    torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Set\n",
    "preds, scores = test(model, train_loader)\n",
    "top_5_map = merge_preds(preds, scores, ids_train, predicted_evidence_train)\n",
    "format_output('train_results.txt',top_5_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dev Set\n",
    "preds, scores = test(model, dev_loader)\n",
    "top_5_map = merge_preds(preds, ids_dev, predicted_evidence_dev)\n",
    "format_output('dev_results.txt', top_5_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set\n",
    "preds, scores = test(model, test_loader)\n",
    "top_5_map = merge_preds(preds, ids_test, predicted_evidence_test)\n",
    "format_output('test_results.txt', top_5_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
